{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNOjLvt4iewUoyCndpEq/WK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSJeong-me/GPT-Insights/blob/main/Tunnel/11-DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## “과부하 상황에서도 일정 수준의 실시간성 유지”를 목표로 한 DQN 기반 자원관리 전략\n",
        "\n",
        "환경(Env)은 시계열을 따라 진행하며, 액션으로 CCTV_Load / Lighting_Controller / Dimming_Controller를 ±10%씩 조정합니다.\n",
        "\n",
        "보상은 지연시간(Delay_Time_ms) 최소화, 안전 임계( CPU≥85, MEM≥90, Fire/Emergency ) 회피, **서비스 유지(너무 과격한 셰딩 억제)**를 함께 반영합니다.\n",
        "\n",
        "Anomaly==1이거나 지연이 너무 커지면 에피소드가 종료됩니다.\n",
        "\n",
        "타깃 네트워크/리플레이 버퍼/허브 로스(Huber)/그라디언트 클리핑 등 DQN 기본기를 갖춘 템플릿"
      ],
      "metadata": {
        "id": "AXInt12sEzmF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eynlgmjHEw_I",
        "outputId": "162472ee-043e-4be6-8f79-7d64858476c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Script written to /content/dqn_resource_manager.py\n",
            "Files that will be created after training:\n",
            "- /content/data/dqn_resource_manager.pt (trained model weights)\n",
            "- /content/data/dqn_training_stats.json (rewards/metrics)\n"
          ]
        }
      ],
      "source": [
        "# Create a ready-to-run PyTorch DQN template that learns a resource-management policy\n",
        "# using the provided /mnt/data/Edge_Device_Training_Data.csv dataset.\n",
        "#\n",
        "# The environment is lightweight and gym-free to avoid extra dependencies.\n",
        "# It treats actions as simple throttling/boosting of controllable loads\n",
        "# (CCTV_Load, Lighting_Controller, Dimming_Controller) with clamps.\n",
        "#\n",
        "# Reward prioritizes: (1) keeping latency low (Delay_Time_ms), (2) avoiding\n",
        "# anomaly conditions (CPU>=85, MEM>=90, Fire/Emergency), and (3) preserving\n",
        "# service/throughput (discouraging aggressive shedding).\n",
        "#\n",
        "# Save script to /mnt/data/dqn_resource_manager.py for the user to download.\n",
        "\n",
        "import os, textwrap, json, pandas as pd, numpy as np\n",
        "\n",
        "script_path = \"/content/dqn_resource_manager.py\"\n",
        "\n",
        "code = r'''\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, List, Dict, Any\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# ------------------------------\n",
        "# Config\n",
        "# ------------------------------\n",
        "CSV_PATH = \"/content/Edge_Device_Training_Data.csv\"\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# Control knobs and thresholds (tune as needed)\n",
        "CPU_HIGH = 85.0\n",
        "MEM_HIGH = 90.0\n",
        "LAT_TARGET_MS = 80.0     # \"real-time-ish\" target\n",
        "LAT_HARD_MAX_MS = 250.0\n",
        "\n",
        "# Action magnitudes\n",
        "ADJ_STEP = 0.10          # 10% increments\n",
        "\n",
        "# Columns used\n",
        "CTRL_COLS = [\"CCTV_Load\", \"Lighting_Controller\", \"Dimming_Controller\"]\n",
        "BIN_COLS  = [\"Generator\", \"Load_Switch\", \"Circuit_Breaker\", \"Jet_Fan\"]  # kept as state only\n",
        "SAFETY_COLS = [\"Fire_Signal\", \"Emergency_Alarm\"]\n",
        "CORE_COLS = [\"CPU_usage\", \"Memory_usage\", \"Delay_Time_ms\"]\n",
        "USE_COLS = [\"CPU_usage\",\"Memory_usage\",\"ATD\",\"ATS\",\"Fire_Signal\",\"Leak_Detector\",\n",
        "            \"Dimming_Controller\",\"Dimming_Addon\",\"Generator\",\"Transformer\",\n",
        "            \"Load_Switch\",\"Circuit_Breaker\",\"Emergency_Alarm\",\"Jet_Fan\",\n",
        "            \"Lighting_Controller\",\"Anemometer\",\"CCTV_Load\",\"Delay_Time_ms\",\"Anomaly\"]\n",
        "\n",
        "# ------------------------------\n",
        "# Utility: MinMax scaler (per column)\n",
        "# ------------------------------\n",
        "class MinMax:\n",
        "    def __init__(self):\n",
        "        self.ranges = {}\n",
        "\n",
        "    def fit(self, df: pd.DataFrame, cols: List[str]):\n",
        "        for c in cols:\n",
        "            v = df[c].astype(float).values\n",
        "            mn, mx = float(np.min(v)), float(np.max(v))\n",
        "            if math.isclose(mx, mn):\n",
        "                mx = mn + 1.0\n",
        "            self.ranges[c] = (mn, mx)\n",
        "\n",
        "    def transform_row(self, row: pd.Series, cols: List[str]) -> np.ndarray:\n",
        "        out = []\n",
        "        for c in cols:\n",
        "            mn, mx = self.ranges[c]\n",
        "            x = float(row[c])\n",
        "            out.append((x - mn) / (mx - mn))\n",
        "        return np.array(out, dtype=np.float32)\n",
        "\n",
        "# ------------------------------\n",
        "# Lightweight Env\n",
        "# ------------------------------\n",
        "class EdgeEnv:\n",
        "    \"\"\"\n",
        "    A simple seq env over the time-series.\n",
        "    Next state advances 1 step in time. Actions modify 'controllable' loads\n",
        "    before evaluating reward. The actual dataset values are used as baseline,\n",
        "    and actions apply a heuristic delta (clamped). This is a pragmatic simulator.\n",
        "    \"\"\"\n",
        "    def __init__(self, df: pd.DataFrame):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.idx = 0\n",
        "        self.n = len(self.df)\n",
        "        self.ctrl_cols = CTRL_COLS\n",
        "        self.state_cols = USE_COLS.copy()\n",
        "        self.scaler = MinMax()\n",
        "        self.scaler.fit(self.df, self.state_cols)\n",
        "\n",
        "        # 7 discrete actions:\n",
        "        # 0: no-op\n",
        "        # 1: CCTV_Load -10%\n",
        "        # 2: Lighting_Controller -10%\n",
        "        # 3: Dimming_Controller -10%\n",
        "        # 4: CCTV_Load +10%\n",
        "        # 5: Lighting_Controller +10%\n",
        "        # 6: Dimming_Controller +10%\n",
        "        self.action_space_n = 7\n",
        "        self.observation_space = len(self.state_cols)\n",
        "\n",
        "        # heuristic coefficients: how control deltas affect CPU and Delay (tunable)\n",
        "        self.alpha_cpu = 0.20    # CPU changes per -10% CCTV/Lighting/Dimming (fractional)\n",
        "        self.beta_lat  = 8.0     # ms change per -10% control\n",
        "        self.gamma_thr = 0.001   # reward for preserving throughput\n",
        "\n",
        "    def _get_obs(self) -> np.ndarray:\n",
        "        row = self.df.iloc[self.idx]\n",
        "        return self.scaler.transform_row(row, self.state_cols)\n",
        "\n",
        "    def _apply_action(self, row: pd.Series, a: int) -> pd.Series:\n",
        "        row = row.copy()\n",
        "        # compute proportional delta for each control\n",
        "        delta = {c: 0.0 for c in self.ctrl_cols}\n",
        "        if a == 1: delta[\"CCTV_Load\"] = -ADJ_STEP\n",
        "        elif a == 2: delta[\"Lighting_Controller\"] = -ADJ_STEP\n",
        "        elif a == 3: delta[\"Dimming_Controller\"] = -ADJ_STEP\n",
        "        elif a == 4: delta[\"CCTV_Load\"] = +ADJ_STEP\n",
        "        elif a == 5: delta[\"Lighting_Controller\"] = +ADJ_STEP\n",
        "        elif a == 6: delta[\"Dimming_Controller\"] = +ADJ_STEP\n",
        "\n",
        "        # apply to control columns with clamping [0,100] for percentages\n",
        "        for c, d in delta.items():\n",
        "            base = float(row[c])\n",
        "            newv = base * (1.0 + d)\n",
        "            newv = max(0.0, min(100.0, newv))\n",
        "            row[c] = newv\n",
        "\n",
        "        # Heuristic effect on CPU and Delay:\n",
        "        # more shedding (negative delta) -> lower CPU and lower latency\n",
        "        shed = -(delta[\"CCTV_Load\"] + delta[\"Lighting_Controller\"] + delta[\"Dimming_Controller\"])  # positive if shedding\n",
        "        cpu = float(row[\"CPU_usage\"]) * (1.0 - self.alpha_cpu * shed)\n",
        "        mem = float(row[\"Memory_usage\"])  # assuming memory reacts slower\n",
        "        delay = float(row[\"Delay_Time_ms\"]) - self.beta_lat * shed\n",
        "\n",
        "        # clamp reasonable bounds\n",
        "        row[\"CPU_usage\"] = max(0.0, min(100.0, cpu))\n",
        "        row[\"Delay_Time_ms\"] = max(0.0, delay)\n",
        "\n",
        "        return row\n",
        "\n",
        "    def reset(self, start_idx: int = None) -> np.ndarray:\n",
        "        self.idx = start_idx if start_idx is not None else random.randint(0, max(0, self.n-2))\n",
        "        return self._get_obs()\n",
        "\n",
        "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, Dict[str, Any]]:\n",
        "        cur = self.df.iloc[self.idx]\n",
        "        cur_adj = self._apply_action(cur, action)\n",
        "\n",
        "        # Reward design\n",
        "        cpu, mem = cur_adj[\"CPU_usage\"], cur_adj[\"Memory_usage\"]\n",
        "        fire, alarm = int(cur_adj[\"Fire_Signal\"]), int(cur_adj[\"Emergency_Alarm\"])\n",
        "        delay = cur_adj[\"Delay_Time_ms\"]\n",
        "        anomaly = int(cur_adj[\"Anomaly\"])\n",
        "\n",
        "        # priorities: keep delay below target, avoid anomaly, keep throughput (sum controls)\n",
        "        r = 0.0\n",
        "        # Latency shaping\n",
        "        r += - max(0.0, (delay - LAT_TARGET_MS)) / 25.0\n",
        "        # Safety penalties\n",
        "        if cpu >= CPU_HIGH: r -= 1.0\n",
        "        if mem >= MEM_HIGH: r -= 1.0\n",
        "        if fire == 1: r -= 2.0\n",
        "        if alarm == 1: r -= 2.0\n",
        "        # Anomaly penalty and early stop\n",
        "        if anomaly == 1: r -= 3.0\n",
        "\n",
        "        # Throughput proxy (prefer higher service)\n",
        "        thr = (cur_adj[\"CCTV_Load\"] + cur_adj[\"Lighting_Controller\"] + cur_adj[\"Dimming_Controller\"]) / 300.0\n",
        "        r += self.gamma_thr * thr\n",
        "\n",
        "        done = False\n",
        "        if anomaly == 1 or delay > LAT_HARD_MAX_MS or self.idx >= self.n - 2:\n",
        "            done = True\n",
        "\n",
        "        # advance time\n",
        "        self.idx += 1\n",
        "        obs = self._get_obs()\n",
        "        info = {\"cpu\": float(cpu), \"mem\": float(mem), \"delay\": float(delay), \"anomaly\": anomaly}\n",
        "        return obs, float(r), done, info\n",
        "\n",
        "# ------------------------------\n",
        "# DQN Components\n",
        "# ------------------------------\n",
        "class QNet(nn.Module):\n",
        "    def __init__(self, in_dim: int, out_dim: int):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, 256), nn.ReLU(),\n",
        "            nn.Linear(256, 128), nn.ReLU(),\n",
        "            nn.Linear(128, out_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "@dataclass\n",
        "class ReplayBuf:\n",
        "    cap: int\n",
        "    s: List[np.ndarray] = None\n",
        "    a: List[int] = None\n",
        "    r: List[float] = None\n",
        "    ns: List[np.ndarray] = None\n",
        "    d: List[bool] = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.s, self.a, self.r, self.ns, self.d = [], [], [], [], []\n",
        "\n",
        "    def push(self, s, a, r, ns, d):\n",
        "        if len(self.s) >= self.cap:\n",
        "            self.s.pop(0); self.a.pop(0); self.r.pop(0); self.ns.pop(0); self.d.pop(0)\n",
        "        self.s.append(s); self.a.append(a); self.r.append(r); self.ns.append(ns); self.d.append(d)\n",
        "\n",
        "    def sample(self, batch: int):\n",
        "        idx = np.random.choice(len(self.s), size=batch, replace=False)\n",
        "        S = torch.tensor(np.array([self.s[i] for i in idx]), dtype=torch.float32)\n",
        "        A = torch.tensor(np.array([self.a[i] for i in idx]), dtype=torch.int64).unsqueeze(1)\n",
        "        R = torch.tensor(np.array([self.r[i] for i in idx]), dtype=torch.float32).unsqueeze(1)\n",
        "        NS= torch.tensor(np.array([self.ns[i] for i in idx]), dtype=torch.float32)\n",
        "        D = torch.tensor(np.array([self.d[i] for i in idx]), dtype=torch.float32).unsqueeze(1)\n",
        "        return S, A, R, NS, D\n",
        "\n",
        "def train_dqn(episodes=10, steps_per_ep=512, batch=64, gamma=0.99,\n",
        "              eps_start=1.0, eps_end=0.05, eps_decay=0.995,\n",
        "              lr=1e-3, buf_cap=20000, target_sync=200):\n",
        "    # Load data\n",
        "    df = pd.read_csv(CSV_PATH)\n",
        "    # Basic cleaning/sorting\n",
        "    ts_col = \"Unnamed: 0\" if \"Unnamed: 0\" in df.columns else None\n",
        "    if ts_col is not None:\n",
        "        try:\n",
        "            df[ts_col] = pd.to_datetime(df[ts_col])\n",
        "            df = df.sort_values(ts_col)\n",
        "        except Exception:\n",
        "            pass\n",
        "    # Keep columns\n",
        "    missing = [c for c in USE_COLS if c not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing columns in CSV: {missing}\")\n",
        "    df = df[USE_COLS].copy()\n",
        "\n",
        "    env = EdgeEnv(df)\n",
        "    q = QNet(env.observation_space, env.action_space_n)\n",
        "    tq = QNet(env.observation_space, env.action_space_n)\n",
        "    tq.load_state_dict(q.state_dict())\n",
        "\n",
        "    opt = optim.AdamW(q.parameters(), lr=lr)\n",
        "    loss_fn = nn.SmoothL1Loss()\n",
        "    rb = ReplayBuf(buf_cap)\n",
        "\n",
        "    eps = eps_start\n",
        "    global_step = 0\n",
        "    stats = {\"ep_reward\": [], \"final_delay\": [], \"final_anomaly\": []}\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        s = env.reset()\n",
        "        ep_r = 0.0\n",
        "        last_info = {}\n",
        "        for t in range(steps_per_ep):\n",
        "            global_step += 1\n",
        "            if random.random() < eps:\n",
        "                a = random.randrange(env.action_space_n)\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    qs = q(torch.tensor(s, dtype=torch.float32).unsqueeze(0))\n",
        "                    a = int(torch.argmax(qs, dim=1).item())\n",
        "\n",
        "            ns, r, done, info = env.step(a)\n",
        "            rb.push(s, a, r, ns, done)\n",
        "            s = ns\n",
        "            ep_r += r\n",
        "            last_info = info\n",
        "\n",
        "            # train\n",
        "            if len(rb.s) >= batch:\n",
        "                S, A, R, NS, D = rb.sample(batch)\n",
        "                with torch.no_grad():\n",
        "                    max_next = tq(NS).max(1, keepdim=True)[0]\n",
        "                    y = R + gamma * (1.0 - D) * max_next\n",
        "                qvals = q(S).gather(1, A)\n",
        "                loss = loss_fn(qvals, y)\n",
        "                opt.zero_grad()\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(q.parameters(), 1.0)\n",
        "                opt.step()\n",
        "\n",
        "                if global_step % target_sync == 0:\n",
        "                    tq.load_state_dict(q.state_dict())\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        eps = max(eps_end, eps * eps_decay)\n",
        "        stats[\"ep_reward\"].append(ep_r)\n",
        "        stats[\"final_delay\"].append(float(last_info.get(\"delay\", np.nan)))\n",
        "        stats[\"final_anomaly\"].append(int(last_info.get(\"anomaly\", 0)))\n",
        "        print(f\"[EP {ep+1:03d}] reward={ep_r:.2f} eps={eps:.3f} delay={stats['final_delay'][-1]:.1f} anom={stats['final_anomaly'][-1]}\")\n",
        "\n",
        "    # Save artifacts\n",
        "    torch.save(q.state_dict(), \"/content/data/dqn_resource_manager.pt\")\n",
        "    with open(\"/content/data/dqn_training_stats.json\", \"w\") as f:\n",
        "        import json; json.dump(stats, f, indent=2)\n",
        "    return stats\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # quick smoke run (adjust episodes for real training)\n",
        "    stats = train_dqn(episodes=5, steps_per_ep=512)\n",
        "    print(\"Training finished. Stats:\", {k: (v[-3:] if isinstance(v, list) else v) for k,v in stats.items()})\n",
        "'''\n",
        "\n",
        "with open(script_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(code)\n",
        "\n",
        "print(f\"Script written to {script_path}\")\n",
        "print(\"Files that will be created after training:\")\n",
        "print(\"- /content/data/dqn_resource_manager.pt (trained model weights)\")\n",
        "print(\"- /content/data/dqn_training_stats.json (rewards/metrics)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/data"
      ],
      "metadata": {
        "id": "pMggcQ4NHbq-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/dqn_resource_manager.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDZBRDbvGbJq",
        "outputId": "f9023ef7-86fa-4358-d832-6d6915da0276"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EP 001] reward=-5.85 eps=0.995 delay=70.6 anom=1\n",
            "[EP 002] reward=-14.74 eps=0.990 delay=153.2 anom=1\n",
            "[EP 003] reward=-5.90 eps=0.985 delay=77.6 anom=1\n",
            "[EP 004] reward=-27.52 eps=0.980 delay=141.7 anom=1\n",
            "[EP 005] reward=-4.68 eps=0.975 delay=97.1 anom=1\n",
            "Training finished. Stats: {'ep_reward': [-5.899677546413392, -27.515160439117018, -4.68222968570311], 'final_delay': [77.60492737015795, 141.66382881500138, 97.0663466186037], 'final_anomaly': [1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## https://chatgpt.com/s/t_68a00b47df348191b3062091eb1f5623"
      ],
      "metadata": {
        "id": "MvMd7b30LC17"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yWKn0-FCGjUT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}